{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Multilabel Classification of Hatespeech Dataset.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTf201ayaA5C"
      },
      "source": [
        "import warnings\n",
        "import nltk\n",
        "from sklearn.metrics import f1_score, accuracy_score, hamming_loss, make_scorer, fbeta_score, multilabel_confusion_matrix,\\\n",
        "    average_precision_score, precision_score, recall_score\n",
        "import numpy as np\n",
        "import time\n",
        "from utilities.preprocess import Preproccesor\n",
        "from utilities.attention_layer import Attention\n",
        "from utilities.helping_functions import create_embedding_matrix\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.models import Sequential, clone_model, model_from_json\n",
        "from keras.optimizers import Adam\n",
        "from keras import Input, Model\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from keras.engine import Layer\n",
        "from keras import backend as K\n",
        "from keras import initializers, regularizers, constraints\n",
        "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, SpatialDropout1D, Bidirectional, Dense, LSTM, Conv1D, MaxPooling1D, Dropout, concatenate, Flatten, add, Conv2D\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold, KFold\n",
        "pd.set_option('max_colwidth', 400)\n",
        "\n",
        "\n",
        "def average_precision_wrapper(y, y_pred, view):\n",
        "    return average_precision_score(y, y_pred.toarray(), average=view)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "S_ljgIk5weaI"
      },
      "source": [
        "hamm_scorer = make_scorer(hamming_loss, greater_is_better=False)\n",
        "ftwo_scorer = make_scorer(fbeta_score, beta=2)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__fqdv2uwVkS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac3e01ad-0416-4a4a-b6f9-693342f23f08"
      },
      "source": [
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords') "
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1WkwUqZ473P"
      },
      "source": [
        "X, yt, y = Preproccesor.load_multi_label_data(True, False) #yt has continuous data, y has binary\n",
        "label_names = [\"violence\",\"directed_vs_generalized\",\"gender\",\"race\",\"national_origin\",\"disability\",\"religion\",\"sexual_orientation\"]"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0T1bzq3aD56",
        "outputId": "09552670-c3e0-4cf6-a088-7ea51865b4cc"
      },
      "source": [
        "import zipfile\n",
        "!wget 'https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-13 15:40:47--  https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 172.67.9.4, 104.22.75.142, 104.22.74.142, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|172.67.9.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1523785255 (1.4G) [application/zip]\n",
            "Saving to: ‘crawl-300d-2M.vec.zip’\n",
            "\n",
            "crawl-300d-2M.vec.z 100%[===================>]   1.42G  12.0MB/s    in 2m 4s   \n",
            "\n",
            "2021-04-13 15:42:52 (11.7 MB/s) - ‘crawl-300d-2M.vec.zip’ saved [1523785255/1523785255]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6Gw8pqU5BeD",
        "outputId": "c4eda118-f1b2-426a-a89c-66379b575285"
      },
      "source": [
        "!wget 'http://nlp.stanford.edu/data/glove.42B.300d.zip'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-13 15:42:52--  http://nlp.stanford.edu/data/glove.42B.300d.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.42B.300d.zip [following]\n",
            "--2021-04-13 15:42:52--  https://nlp.stanford.edu/data/glove.42B.300d.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.42B.300d.zip [following]\n",
            "--2021-04-13 15:42:52--  http://downloads.cs.stanford.edu/nlp/data/glove.42B.300d.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1877800501 (1.7G) [application/zip]\n",
            "Saving to: ‘glove.42B.300d.zip’\n",
            "\n",
            "glove.42B.300d.zip  100%[===================>]   1.75G  4.45MB/s    in 6m 2s   \n",
            "\n",
            "2021-04-13 15:48:54 (4.95 MB/s) - ‘glove.42B.300d.zip’ saved [1877800501/1877800501]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T423urSl5FG9",
        "outputId": "25ff492b-bbdf-4e8d-ede3-5eeb920b9857"
      },
      "source": [
        "with zipfile.ZipFile(\"/content/crawl-300d-2M.vec.zip\", \"r\") as zip_ref:\n",
        "    zip_ref.extractall()\n",
        "    print(zip_ref.filelist)\n",
        "with zipfile.ZipFile(\"/content/glove.42B.300d.zip\", \"r\") as zip_ref:\n",
        "    zip_ref.extractall()\n",
        "    print(zip_ref.filelist)\n",
        "\n",
        "del zip_ref"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[<ZipInfo filename='crawl-300d-2M.vec' compress_type=deflate filemode='-rw-r--r--' file_size=4514687127 compress_size=1523784963>]\n",
            "[<ZipInfo filename='glove.42B.300d.txt' compress_type=deflate filemode='-rw-rw-r--' file_size=5025028820 compress_size=1877800207>]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "fpIelS9AaFdD"
      },
      "source": [
        "!rm '/content/crawl-300d-2M.vec.zip'\n",
        "!rm '/content/glove.42B.300d.zip'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "p6gmfH8eaGt6"
      },
      "source": [
        "embedding_path1 = \"/content/embeddings/crawl-300d-2M.vec\" #FastText\n",
        "embedding_path2 = \"/content/embeddings/glove.42B.300d.txt\" #Glove 300d\n",
        "embed_size = 300"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "5Ejp2HjdaLov"
      },
      "source": [
        "n_fold = 10\n",
        "folds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=7)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "EGFFWC_GkJHK"
      },
      "source": [
        "def my_hamming_loss(y_true, y_pred):\n",
        "    print(y_true, y_pred)\n",
        "    y_true = K.cast(y_true, dtype='float32')\n",
        "    y_pred = K.cast(y_pred, dtype='float32')\n",
        "    print(y_true, y_pred)\n",
        "    hamming_loss(y_true, y_pred)\n",
        "\n",
        "    return K.mean(diff, axis=-1)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "B8_7uXlXqE7Z"
      },
      "source": [
        "# Binary Relevance\n",
        "def build_model1(X_train, y_train, X_valid, y_valid, max_len, max_features, embed_size, embedding_matrix, lr=0.0, lr_d=0.0, spatial_dr=0.0, dense_units=128, conv_size=128, dr=0.2, patience=3, fold_id=1):\n",
        "    file_path = f\"best_model_fold_{fold_id}.hdf5\"\n",
        "    check_point = ModelCheckpoint(\n",
        "        file_path, monitor=\"val_loss\", verbose=1, save_best_only=True, mode=\"min\")\n",
        "    early_stop = EarlyStopping(\n",
        "        monitor=\"val_loss\", mode=\"min\", patience=patience)\n",
        "    main_input = Input(shape=(max_len,), name='main_input')\n",
        "    x = (Embedding(max_features + 1, embed_size*2, input_length=max_len,\n",
        "                   weights=[embedding_matrix], trainable=False))(main_input)\n",
        "    x = SpatialDropout1D(0.4)(x)\n",
        "    x = Bidirectional(LSTM(150, return_sequences=True))(x)\n",
        "    x = Bidirectional(LSTM(150, return_sequences=True))(x)\n",
        "    hidden = concatenate([\n",
        "        Attention(max_len)(x),\n",
        "        GlobalMaxPooling1D()(x),\n",
        "    ])\n",
        "    hidden = Dense(1024, activation='selu')(hidden)\n",
        "    hidden = Dropout(0.4)(hidden)\n",
        "    hidden = Dense(512, activation='selu')(hidden)\n",
        "    hidden = Dropout(0.4)(hidden)\n",
        "    hidden1 = Dense(128, activation='selu')(hidden)\n",
        "    output_lay1 = Dense(8, activation='sigmoid')(hidden1)\n",
        "    model = Model(inputs=[main_input], outputs=output_lay1)\n",
        "    model.compile(loss=\"binary_crossentropy\", optimizer=Adam(\n",
        "        lr=lr, decay=lr_d), metrics=['binary_accuracy'])\n",
        "    from keras.utils import plot_model\n",
        "    plot_model(model, to_file='model1.png')\n",
        "    model2 = Model(inputs=[main_input], outputs=output_lay1)\n",
        "    model.fit(X_train, y_train, batch_size=16, epochs=50, validation_data=(\n",
        "        X_valid, y_valid), verbose=1, callbacks=[early_stop, check_point])\n",
        "    model2.load_weights(file_path)\n",
        "    model2.compile(loss=\"binary_crossentropy\", optimizer=Adam(\n",
        "        lr=lr, decay=lr_d), metrics=['binary_accuracy'])\n",
        "    return model2"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "oreG7FWTRa3a"
      },
      "source": [
        "# Classifier Chains\n",
        "def build_model2(X_train, y_train, X_valid, y_valid, max_len, max_features, embed_size, embedding_matrix, lr=0.0, lr_d=0.0, spatial_dr=0.0, dense_units=128, conv_size=128, dr=0.2, patience=3, fold_id=1):\n",
        "    file_path = f\"best_model_fold_{fold_id}.hdf5\"\n",
        "    check_point = ModelCheckpoint(\n",
        "        file_path, monitor=\"val_loss\", verbose=1, save_best_only=True, mode=\"min\")\n",
        "    early_stop = EarlyStopping(\n",
        "        monitor=\"val_loss\", mode=\"min\", patience=patience)\n",
        "    main_input = Input(shape=(max_len,), name='main_input')\n",
        "    x = (Embedding(max_features + 1, embed_size*2, input_length=max_len,\n",
        "                   weights=[embedding_matrix], trainable=False))(main_input)\n",
        "    x = SpatialDropout1D(0.5)(x)\n",
        "    x = Bidirectional(LSTM(150, return_sequences=True))(x)\n",
        "    x = Bidirectional(LSTM(150, return_sequences=True))(x)\n",
        "    hidden = concatenate([\n",
        "        Attention(max_len)(x),\n",
        "        GlobalMaxPooling1D()(x),\n",
        "    ])\n",
        "    hidden = Dense(1024, activation='selu')(hidden)\n",
        "    hidden = Dropout(0.5)(hidden)\n",
        "    hidden = Dense(512, activation='selu')(hidden)\n",
        "    hidden = Dropout(0.5)(hidden)\n",
        "    hidden1 = Dense(128, activation='selu')(hidden)\n",
        "    output_lay1 = Dense(1, activation='selu')(hidden1)\n",
        "    hidden2 = concatenate([hidden1, output_lay1])\n",
        "    output_lay2 = Dense(1, activation='selu')(hidden2)\n",
        "    hidden3 = concatenate([hidden2, output_lay2])\n",
        "    output_lay3 = Dense(1, activation='selu')(hidden3)\n",
        "    hidden4 = concatenate([hidden3, output_lay3])\n",
        "    output_lay4 = Dense(1, activation='selu')(hidden4)\n",
        "    hidden5 = concatenate([hidden4, output_lay4])\n",
        "    output_lay5 = Dense(1, activation='selu')(hidden5)\n",
        "    hidden6 = concatenate([hidden5, output_lay5])\n",
        "    output_lay6 = Dense(1, activation='selu')(hidden6)\n",
        "    hidden7 = concatenate([hidden6, output_lay6])\n",
        "    output_lay7 = Dense(1, activation='selu')(hidden7)\n",
        "    hidden8 = concatenate([hidden7, output_lay7])\n",
        "    output_lay8 = Dense(1, activation='selu')(hidden8)\n",
        "\n",
        "    hidden_l = concatenate([output_lay1, output_lay2, output_lay3, output_lay4, output_lay5, output_lay6,\n",
        "                            output_lay7, output_lay8])\n",
        "    hidden_l = Dropout(0.5)(hidden_l)\n",
        "    output_layer = Dense(8, activation='sigmoid')(hidden_l)\n",
        "\n",
        "    model = Model(inputs=[main_input], outputs=output_layer)\n",
        "    model.compile(loss=\"binary_crossentropy\", optimizer=Adam(\n",
        "        lr=lr, decay=lr_d), metrics=['binary_accuracy', 'categorical_accuracy'])\n",
        "    from keras.utils import plot_model\n",
        "    plot_model(model, to_file='model2.png')\n",
        "    model2 = Model(inputs=[main_input], outputs=output_layer)\n",
        "    model.fit(X_train, y_train, batch_size=32, epochs=50, validation_data=(\n",
        "        X_valid, y_valid), verbose=1, callbacks=[early_stop, check_point])\n",
        "    model2.load_weights(file_path)\n",
        "    model2.compile(loss=\"binary_crossentropy\", optimizer=Adam(\n",
        "        lr=lr, decay=lr_d), metrics=['binary_accuracy', 'categorical_accuracy'])\n",
        "    return model2"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPes1aSa-yZu"
      },
      "source": [
        "!pip install iterative-stratification"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTkSEEkfaSYS",
        "outputId": "e47ee207-0ec2-449b-9e2e-85ec0d52a22b"
      },
      "source": [
        "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
        "\n",
        "max_features = 50000\n",
        "scores = {}\n",
        "scores = {}\n",
        "scores.setdefault('test_F1_example', [])\n",
        "scores.setdefault('test_F1_macro', [])\n",
        "scores.setdefault('test_F1_micro', [])\n",
        "scores.setdefault('test_precision_example', [])\n",
        "scores.setdefault('test_precision_macro', [])\n",
        "scores.setdefault('test_precision_micro', [])\n",
        "scores.setdefault('test_recall_example', [])\n",
        "scores.setdefault('test_recall_macro', [])\n",
        "scores.setdefault('test_recall_micro', [])\n",
        "scores.setdefault('test_average_precision_macro', [])\n",
        "scores.setdefault('test_average_precision_micro', [])\n",
        "scores.setdefault('test_Accuracy', [])\n",
        "scores.setdefault('test_Hamm', [])\n",
        "cm = []\n",
        "mskf = MultilabelStratifiedKFold(n_splits=10, random_state=0)\n",
        "fold_n = 0\n",
        "save_ys = []\n",
        "save_yt = []\n",
        "max_len = 150\n",
        "embed_size = 150\n",
        "embma = 1\n",
        "name = \"Mixed\"\n",
        "\n",
        "for train_index, test_index in mskf.split(X, y):\n",
        "    print('Fold', fold_n, 'started at', time.ctime())\n",
        "    X_train, X_valid = X[train_index], X[test_index]\n",
        "    y_train, y_valid = y[train_index], y[test_index]\n",
        "    tk = Tokenizer(lower=True, filters='',\n",
        "                   num_words=max_features, oov_token=True)\n",
        "    tk.fit_on_texts(X_train)\n",
        "    train_tokenized = tk.texts_to_sequences(X_train)\n",
        "    valid_tokenized = tk.texts_to_sequences(X_valid)\n",
        "    X_train = pad_sequences(train_tokenized, maxlen=max_len)\n",
        "    X_valid = pad_sequences(valid_tokenized, maxlen=max_len)\n",
        "    embedding_matrix = create_embedding_matrix(embma, tk, max_features)\n",
        "\n",
        "    model = build_model2(X_train, y_train, X_valid, y_valid, max_len, max_features, embed_size, embedding_matrix,lr=1e-3, lr_d=0, spatial_dr=0.1, dense_units=128, conv_size=128, dr=0.1, patience=10, fold_id=fold_n)\n",
        "\n",
        "    fold_n = fold_n + 1\n",
        "    yT = model.predict(X_valid)\n",
        "    y_preds = []\n",
        "    for yt in yT:  # Don't do this if you throw them with continuous values\n",
        "        yi = []\n",
        "        for i in yt:\n",
        "            if i >= 0.5:\n",
        "                yi.append(int(1))\n",
        "            else:\n",
        "                yi.append(int(0))\n",
        "        y_preds.append(yi)\n",
        "    y_preds = np.array(y_preds)\n",
        "    scores['test_F1_example'].append(\n",
        "        f1_score(y_valid, y_preds, average='samples'))\n",
        "    scores['test_F1_macro'].append(f1_score(y_valid, y_preds, average='macro'))\n",
        "    scores['test_F1_micro'].append(f1_score(y_valid, y_preds, average='micro'))\n",
        "    scores['test_precision_example'].append(\n",
        "        precision_score(y_valid, y_preds, average='samples'))\n",
        "    scores['test_precision_macro'].append(\n",
        "        precision_score(y_valid, y_preds, average='macro'))\n",
        "    scores['test_precision_micro'].append(\n",
        "        precision_score(y_valid, y_preds, average='micro'))\n",
        "    scores['test_recall_example'].append(\n",
        "        recall_score(y_valid, y_preds, average='samples'))\n",
        "    scores['test_recall_macro'].append(\n",
        "        recall_score(y_valid, y_preds, average='macro'))\n",
        "    scores['test_recall_micro'].append(\n",
        "        recall_score(y_valid, y_preds, average='micro'))\n",
        "    scores['test_average_precision_macro'].append(\n",
        "        average_precision_score(y_valid, y_preds, average='macro'))\n",
        "    scores['test_average_precision_micro'].append(\n",
        "        average_precision_score(y_valid, y_preds, average='micro'))\n",
        "    scores['test_Accuracy'].append(accuracy_score(y_valid, y_preds))\n",
        "    scores['test_Hamm'].append(hamming_loss(y_valid, y_preds))\n",
        "\n",
        "f = open(\"../results/setZ.txt\", \"a+\")\n",
        "f.write(\"{:<7} | {:<7} {:<7} {:<7} {:<7} {:<7} {:<7} {:<7} {:<7} {:<7} {:<7} {:<7} {:<7} {:<7} \\n\".format(str(name)[:7],\n",
        "        str('%.4f' % (\n",
        "            sum(scores['test_F1_example'])/10)),\n",
        "        str('%.4f' % (\n",
        "            sum(scores['test_F1_macro'])/10)),\n",
        "        str('%.4f' % (\n",
        "            sum(scores['test_F1_micro']) / 10)),\n",
        "        str('%.4f' % (\n",
        "            sum(scores['test_precision_example']) / 10)),\n",
        "        str('%.4f' % (\n",
        "            sum(scores['test_precision_macro']) / 10)),\n",
        "        str('%.4f' % (\n",
        "            sum(scores['test_precision_micro']) / 10)),\n",
        "        str('%.4f' % (\n",
        "            sum(scores['test_recall_example']) / 10)),\n",
        "        str('%.4f' % (\n",
        "            sum(scores['test_recall_macro']) / 10)),\n",
        "        str('%.4f' % (\n",
        "            sum(scores['test_recall_micro']) / 10)),\n",
        "        str('%.4f' % (\n",
        "            sum(scores['test_average_precision_macro'])/10)),\n",
        "        str('%.4f' % (\n",
        "            sum(scores['test_average_precision_micro'])/10)),\n",
        "        str('%.4f' % (\n",
        "            sum(scores['test_Accuracy'])/10)),\n",
        "        str('%.4f' % (sum(scores['test_Hamm'])/10))))\n",
        "f.close()"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
            "  FutureWarning\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOpCaBSd-Ksp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}